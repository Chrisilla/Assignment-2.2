Explain in detail :
1.HDFS :
	Hadoop Distributed File System

	*It is used for storage in hadoop.
	*It is developed using  Java 
	*It consists of a namenode which is called the master. It controls the namespace of filesystem.
	*It runs on commodity hardware.
	*There are many data nodes that manages the storage to the nodes they run.
	*Hadoop supports shell-like commands to interact with HDFS directly.
	*HDFS stores files across the collection of servers in the form of clusters.	
	
	Pros:
	# It finds and fixes the hardware problems.
	#*HDFS is a 
		@distributed storage system
		@scalable
		@fault-tolerant
		


2.Hadoop cluster :	
	*A computer cluster used for Hadoop is called Hadoop Cluster. 
	
	*Hadoop cluster boosts the speed of data analysis applications.
 	
	*Hadoop clusters are scalable and highly resistant to failures.
	*Hadoop cluster is designed specifically for storing and analyzing huge amounts of 
	unstructured data.
	*Hadoop clusters are composed of three types of node: 
		-master nodes
		-worker nodes
		-client nodes. 
	*Hadoop cluster will run in low cost commodity hardwares.
	*Clusters of three or more machines typically use a single NameNode and ResourceManager
 	with all the other nodes as slave nodes.

3.HDFS Blocks :

	*Hadoop distributed file system  stores the data in the form of blocks.
	*Hard Disk has concentric circles which form tracks.
	*HDFS blocks are larger in size in order to reduce the cost of seek time.
	*Blocks are easy to replicate and there by providing fault tolerance and hign availability.
	*The default replication factor of blocks is 3.
	*The default size of hdfs block is 64MB.	
	*Blocks are linked to the file through INode. Each block is given a timestamp 
	  that is used to determine whether a replica is current. 